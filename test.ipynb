{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch  # 导入torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "label_to_index = {'0': 0, '1': 1}  # 需要根据实际情况调整\n",
    "\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, audio_processor, max_length=512):\n",
    "        self.data = pd.read_csv(csv_file, sep=\"\\t\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.audio_processor = audio_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        label_name = row['label']\n",
    "        label = torch.tensor(label_name, dtype=torch.long)  # 使用映射转换标签\n",
    "        text = row['dialogue']\n",
    "        audio_path = row['new_audio_segments_path']\n",
    "        visual_path = row['visual_segment_path']\n",
    "\n",
    "        # 处理视觉特征\n",
    "        visual_features = np.loadtxt(visual_path, delimiter=',', skiprows=1)[:, 4:].astype(np.float32)  # 转换为 float32\n",
    "        scaler = StandardScaler()\n",
    "        # 对视觉特征进行标准化\n",
    "        visual_features = scaler.fit_transform(visual_features)\n",
    "\n",
    "\n",
    "        # 处理文本\n",
    "        text_input = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        text_input = {key: val.squeeze(0) for key, val in text_input.items()}  # 转换为相同的数据类型\n",
    "\n",
    "        audio_segments_data = pd.read_csv(audio_path, sep=\"\\t\")\n",
    "        audio_values = []\n",
    "        for _, segment_row in audio_segments_data.iterrows():\n",
    "            audio_path = segment_row['audio_segment_path']\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            segment_values = self.audio_processor(waveform.squeeze().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "            audio_values.append(segment_values.squeeze(0))  # 移除批次维度\n",
    "\n",
    "        # 堆叠音频片段\n",
    "        audio_values = torch.stack(audio_values, dim=0)  # 形状为 [num_segments, sequence_length]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'audio_values': audio_values,\n",
    "            'text_input': text_input,\n",
    "            'visual_features': visual_features,\n",
    "            'label': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, Wav2Vec2Processor\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./transformers/roberta')\n",
    "wav2vec2processor = Wav2Vec2Processor.from_pretrained(\"./transformers/wav2vecprocessor\")\n",
    "\n",
    "# 假设你已经有了 AudioTextDataset 的定义\n",
    "full_dataset = AudioTextDataset('train.csv', tokenizer, wav2vec2processor)\n",
    "\n",
    "# 定义数据集的大小\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(total_size * 0.80)\n",
    "test_size = int(total_size * 0.05)\n",
    "valid_size = total_size - train_size - test_size  # 保证总和为total_size\n",
    "\n",
    "# 随机划分数据集\n",
    "train_dataset, test_dataset, valid_dataset = random_split(full_dataset, [train_size, test_size, valid_size])\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 现在你有三个独立的 DataLoader：train_loader, test_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 160000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(train_loader))[1]['audio_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from transformers import Wav2Vec2Model, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualModal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(VisualModal, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)  # 第一个全连接层\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, output_dim)  # 第二个全连接层\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, visual_input):\n",
    "        # 处理视觉特征\n",
    "        visual_output, _ = self.lstm(visual_input)\n",
    "        visual_output = visual_output[:, -1, :]  # 获取 LSTM 的最后一个时间步的输出\n",
    "        \n",
    "        # 通过两个全连接层\n",
    "        visual_output = self.fc1(visual_output)\n",
    "        visual_output = self.relu(visual_output)\n",
    "        visual_output = self.fc2(visual_output)\n",
    "        return visual_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads, output_dim):\n",
    "        super(CrossEncoder, self).__init__()\n",
    "        # 为每种交互创建一个独立的多头注意力模块\n",
    "        self.attn_audio_text = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads)\n",
    "        self.attn_text_audio = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads)\n",
    "\n",
    "        self.self_attn1 = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads)\n",
    "        self.self_attn2 = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads)\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(feature_dim * 6, output_dim)  # 假设拼接后的特征用于一个线性层\n",
    "\n",
    "    def forward(self, audio_feature, text_feature, visual_feature):\n",
    "        # 分别使用独立的注意力模块\n",
    "        attn_output_1, _ = self.attn_audio_text(query=audio_feature.unsqueeze(0), key=text_feature.unsqueeze(0), value=text_feature.unsqueeze(0))\n",
    "        attn_output_3, _ = self.attn_text_audio(query=text_feature.unsqueeze(0), key=audio_feature.unsqueeze(0), value=audio_feature.unsqueeze(0))\n",
    "\n",
    "        self_attn_output_1, _ = self.self_attn1(query=attn_output_1, key=attn_output_1, value=attn_output_1)\n",
    "        self_attn_output_2, _ = self.self_attn2(query=attn_output_3, key=attn_output_3, value=attn_output_3)\n",
    "\n",
    "        \n",
    "        return self_attn_output_1.squeeze(0), self_attn_output_2.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondCrossEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SecondCrossEncoder, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=num_heads)\n",
    "        self.ffn = FeedForwardNetwork(feature_dim, hidden_dim, output_dim, dropout_rate)\n",
    "\n",
    "    def forward(self, audio_feature, text_feature, visual_feature, attn_output_1, attn_output_2, attn_output_3, attn_output_4, attn_output_5, attn_output_6):\n",
    "        # 使用cross attention提取三模态之间的信息\n",
    "        triple_attn_output_1, _ = self.multihead_attn(query=attn_output_2.unsqueeze(0), key=attn_output_1.unsqueeze(0), value=attn_output_1.unsqueeze(0))\n",
    "        triple_attn_output_2, _ = self.multihead_attn(query=attn_output_3.unsqueeze(0), key=attn_output_4.unsqueeze(0), value=attn_output_4.unsqueeze(0))\n",
    "        triple_attn_output_3, _ = self.multihead_attn(query=attn_output_5.unsqueeze(0), key=attn_output_6.unsqueeze(0), value=attn_output_6.unsqueeze(0))\n",
    "\n",
    "        # 添加残差连接\n",
    "        triple_attn_output_1 = triple_attn_output_1.squeeze(0) + audio_feature\n",
    "        triple_attn_output_2 = triple_attn_output_2.squeeze(0) + text_feature\n",
    "        triple_attn_output_3 = triple_attn_output_3.squeeze(0) + visual_feature\n",
    "\n",
    "        # 通过前馈神经网络\n",
    "        triple_attn_output_1 = self.ffn(triple_attn_output_1)\n",
    "        triple_attn_output_2 = self.ffn(triple_attn_output_2)\n",
    "        triple_attn_output_3 = self.ffn(triple_attn_output_3)\n",
    "\n",
    "        return triple_attn_output_1, triple_attn_output_2, triple_attn_output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, lstm_output_dim):\n",
    "        super(AudioModel, self).__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"./transformers/wav2vec2\")\n",
    "        self.lstm = nn.LSTM(input_size=self.wav2vec.config.hidden_size, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, lstm_output_dim)\n",
    "\n",
    "        # 冻结预训练模型的参数\n",
    "        for param in self.wav2vec.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        # input_values 的形状应该是 [batch_size, num_segments, sequence_length]\n",
    "        batch_size, num_segments, _ = input_values.shape\n",
    "        wav2vec_features = []\n",
    "\n",
    "        for i in range(num_segments):\n",
    "            segment = input_values[:, i, :]\n",
    "            with torch.no_grad():\n",
    "                segment_features = self.wav2vec(input_values=segment).last_hidden_state\n",
    "            wav2vec_features.append(segment_features)\n",
    "\n",
    "        # 将所有片段的特征拼接起来\n",
    "        wav2vec_features = torch.cat(wav2vec_features, dim=1)  # 形状为 [batch_size, num_segments * sequence_length, hidden_size]\n",
    "\n",
    "        # 通过 LSTM\n",
    "        lstm_output, _ = self.lstm(wav2vec_features)\n",
    "        # 获取 LSTM 的最后一步输出\n",
    "        last_hidden_state = lstm_output[:, -1, :]\n",
    "        output = self.fc(last_hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "768\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 512\n",
    "visual_lstm_final_layer_dim = 768\n",
    "lstm_output_dim = 768\n",
    "output_dim = 2\n",
    "visual_feature_dim = 136  # 假设您的视觉特征维度为136\n",
    "feedforward_hidden_dim = 128  # 假设您想要的 LSTM 隐藏层维度为128\n",
    "feature_dim = 768  # 假设特征维度是768\n",
    "num_heads = 8  # 假设使用8个注意力头\n",
    "\n",
    "audio_model = AudioModel(hidden_dim, lstm_output_dim)\n",
    "\n",
    "# 加载预训练的文本模型\n",
    "text_model = RobertaModel.from_pretrained('./transformers/roberta-base', num_labels=2)\n",
    "\n",
    "visual_model = VisualModal(input_dim=visual_feature_dim, hidden_dim=hidden_dim, output_dim=visual_lstm_final_layer_dim)\n",
    "\n",
    "feedforward_model1 = FeedForwardNetwork(visual_lstm_final_layer_dim, hidden_dim, feedforward_hidden_dim)\n",
    "feedforward_model2 = FeedForwardNetwork(visual_lstm_final_layer_dim, hidden_dim, feedforward_hidden_dim)\n",
    "# 实例化cross_encoder和second_cross_encoder\n",
    "cross_encoder = CrossEncoder(feature_dim=feature_dim, num_heads=num_heads, output_dim=feature_dim)\n",
    "# second_cross_encoder = SecondCrossEncoder(feature_dim=feature_dim, num_heads=num_heads, hidden_dim=hidden_dim, output_dim=feature_dim, dropout_rate=0.2)\n",
    "\n",
    "# # 冻结预训练模型的参数\n",
    "for param in audio_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "audio_batch = batch['audio_values']\n",
    "text_batch = batch['text_input']\n",
    "visual_input = batch['visual_features']\n",
    "\n",
    "with torch.no_grad():\n",
    "    audio_output = audio_model(input_values=audio_batch)\n",
    "    text_output = text_model(**text_batch).last_hidden_state\n",
    "    visual_output = visual_model(visual_input)\n",
    "    audio_output_dim = audio_output.shape[-1]\n",
    "    text_output_dim = text_output.shape[-1]    \n",
    "    visual_output_dim = visual_output.shape[-1]\n",
    "\n",
    "print(audio_output_dim)\n",
    "print(text_output_dim)\n",
    "print(visual_output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultimodalFusionModel(nn.Module):\n",
    "    def __init__(self, audio_model, text_model, visual_model, cross_encoder, audio_output_dim, text_output_dim, visual_output_dim, hidden_dim, output_dim, feedforward_model1, feedforward_model2, dropout_rate=0.3):\n",
    "        super(MultimodalFusionModel, self).__init__()\n",
    "        self.audio_model = audio_model\n",
    "        self.text_model = text_model\n",
    "        self.visual_model = visual_model\n",
    "        self.cross_encoder = cross_encoder\n",
    "        self.feedforward_model1 = feedforward_model1\n",
    "        self.feedforward_model2 = feedforward_model2\n",
    "        # 定义全连接层和其他层\n",
    "        self.fc1 = nn.Linear(512, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim // 4)\n",
    "        self.fc4 = nn.Linear(hidden_dim // 4, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, audio_input, text_input, visual_input):\n",
    "        # 获取音频模型和文本模型的最后一层输出\n",
    "        audio_output = self.audio_model(input_values=audio_input)\n",
    "        text_output = self.text_model(**text_input).last_hidden_state[:, -1, :]\n",
    "        visual_output = self.visual_model(visual_input)\n",
    "\n",
    "        # 进行cross attention和second cross attention\n",
    "        attn_output_1, attn_output_3 = self.cross_encoder(audio_output, text_output, visual_output)\n",
    "\n",
    "        output1 = self.feedforward_model1(attn_output_1)\n",
    "        output2 = self.feedforward_model2(attn_output_3)\n",
    "    \n",
    "        # 拼接三个模态的输出\n",
    "        combined_output = torch.cat((output1, output2), dim=1)\n",
    "        \n",
    "        # 通过全连接层和其他层\n",
    "        x = self.fc1(combined_output)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "fusion_model = MultimodalFusionModel(audio_model, text_model, visual_model, cross_encoder, audio_output_dim, text_output_dim, visual_output_dim, hidden_dim, output_dim, feedforward_model1, feedforward_model2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 17177730\n"
     ]
    }
   ],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_trainable_params = count_trainable_params(fusion_model)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:02<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 137.8402, Train Accuracy: 0.5775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:11<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Loss: 0.7000, Accuracy: 0.3974, Precision: 0.5518, Recall: 0.5171, F1 Score: 0.3480, AUC: 0.6250\n",
      "Saved new best model with AUC: 0.6250 to best_model.pth\n",
      "Saved ROC curve to roc_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 137.6619, Train Accuracy: 0.6038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Validation Loss: 0.6314, Accuracy: 0.6376, Precision: 0.4875, Recall: 0.4993, F1 Score: 0.4005, AUC: 0.6498\n",
      "Saved new best model with AUC: 0.6498 to best_model.pth\n",
      "Saved ROC curve to roc_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:02<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 137.4883, Train Accuracy: 0.6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Validation Loss: 0.6576, Accuracy: 0.6594, Precision: 0.6181, Recall: 0.5972, F1 Score: 0.5988, AUC: 0.6253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 137.3400, Train Accuracy: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Validation Loss: 0.6436, Accuracy: 0.6769, Precision: 0.6412, Recall: 0.6135, F1 Score: 0.6163, AUC: 0.6567\n",
      "Saved new best model with AUC: 0.6567 to best_model.pth\n",
      "Saved ROC curve to roc_curve.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:02<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 137.2015, Train Accuracy: 0.6226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:10<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Validation Loss: 0.6557, Accuracy: 0.6725, Precision: 0.6362, Recall: 0.6182, F1 Score: 0.6216, AUC: 0.6560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 137.0809, Train Accuracy: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:11<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Validation Loss: 0.6269, Accuracy: 0.6769, Precision: 0.6482, Recall: 0.5892, F1 Score: 0.5822, AUC: 0.6527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [01:02<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 136.9725, Train Accuracy: 0.6382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:11<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Validation Loss: 0.6223, Accuracy: 0.6769, Precision: 0.6501, Recall: 0.5865, F1 Score: 0.5776, AUC: 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 68/77 [00:55<00:07,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = AdamW(fusion_model.parameters(), lr=2e-5)\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    l1_lambda = 1e-5\n",
    "    # 使用tqdm包装dataloader以显示进度条\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        audio_batch = batch['audio_values'].squeeze(1)\n",
    "        audio_input = audio_batch.to(device)\n",
    "\n",
    "        text_input = {key: val.to(device) for key, val in batch['text_input'].items()}\n",
    "\n",
    "        visual_input = batch['visual_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(audio_input, text_input,visual_input)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 计算L1正则化损失\n",
    "        l1_norm = sum(p.abs().sum() for p in fusion_model.parameters())\n",
    "        loss += l1_lambda * l1_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = correct_predictions.double() / len(dataloader.dataset)\n",
    "    return avg_loss, avg_accuracy\n",
    "    \n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []  # Collect probabilities for ROC curve\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        audio_batch = batch['audio_values'].squeeze(1)\n",
    "        audio_input = audio_batch.to(device)\n",
    "        text_input = {key: val.to(device) for key, val in batch['text_input'].items()}\n",
    "        visual_input = batch['visual_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(audio_input, text_input, visual_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs.softmax(dim=1)[:, 1].cpu().numpy())  # Assuming binary classification for ROC\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, fpr, tpr, roc_auc\n",
    "\n",
    "# 训练和评估循环\n",
    "num_epochs = 10\n",
    "best_f1 = 0.0\n",
    "best_accuracy = 0.0\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []  # Collect probabilities for ROC curve\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        audio_batch = batch['audio_values'].squeeze(1)\n",
    "        audio_input = audio_batch.to(device)\n",
    "        text_input = {key: val.to(device) for key, val in batch['text_input'].items()}\n",
    "        visual_input = batch['visual_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(audio_input, text_input, visual_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(outputs.softmax(dim=1)[:, 1].cpu().numpy())  # Assuming binary classification for ROC\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, fpr, tpr, roc_auc\n",
    "\n",
    "best_auc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(fusion_model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n",
    "    \n",
    "    val_loss, val_acc, val_precision, val_recall, val_f1, fpr, tpr, roc_auc = evaluate(fusion_model, valid_loader, criterion)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}, AUC: {roc_auc:.4f}')\n",
    "    \n",
    "    # Save the best model and plot ROC curve\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "        best_model_path = 'best_model.pth'\n",
    "        torch.save(fusion_model.state_dict(), best_model_path)\n",
    "        print(f\"Saved new best model with AUC: {best_auc:.4f} to {best_model_path}\")\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        roc_curve_path = 'roc_curve.png'\n",
    "        plt.savefig(roc_curve_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved ROC curve to {roc_curve_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
